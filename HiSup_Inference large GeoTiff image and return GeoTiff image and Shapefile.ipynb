{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wvlq4PekSIyJ"
   },
   "source": [
    "# HiSup - Inference large GeoTiff image and return prediction mask in GeoTiff and extracted buildings in Shapefile\n",
    "\n",
    "This work is based on https://github.com/SarahwXU/HiSup.<br> \n",
    "I added several blocks to enable creating prediction results with spatial information.Feel free to reuse it for your own application.<br>\n",
    "Here you can use fine-tuned model from your own dataset.<br>\n",
    "To run this Jupyter notebook, you need to preinstall the required packages as instructed by HiSup GitHub.<br>\n",
    "Change the path for your own application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate hisup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jnVks4XDQdto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/shared-nvme/HiSup-main\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdemo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inference_no_patching, inference_with_patching\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhisup\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cfg\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhisup\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_pretrained_model\n",
      "File \u001b[0;32m~/shared-nvme/HiSup-main/scripts/demo.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Define the path\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from scripts.demo import inference_no_patching, inference_with_patching\n",
    "from hisup.config import cfg\n",
    "from hisup.detector import get_pretrained_model\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z4QJ82NzWtH"
   },
   "source": [
    "### Inference Functions from HiSup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BWDeE58zZz8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io\n",
    "from hisup.config import cfg\n",
    "from hisup.detector import get_pretrained_model\n",
    "from hisup.dataset.build import build_transform\n",
    "from hisup.utils.comm import to_single_device\n",
    "from hisup.utils.visualizer import show_polygons\n",
    "\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "from skimage.measure import label, regionprops\n",
    "from hisup.utils.polygon import generate_polygon, juncs_in_bbox\n",
    "from hisup.utils.visualizer import viz_inria\n",
    "\n",
    "from hisup.config import cfg\n",
    "from hisup.detector import BuildingDetector\n",
    "from hisup.dataset import build_train_dataset\n",
    "from hisup.utils.comm import to_single_device\n",
    "from hisup.solver import make_lr_scheduler, make_optimizer\n",
    "from hisup.utils.logger import setup_logger\n",
    "from hisup.utils.miscellaneous import save_config\n",
    "from hisup.utils.metric_logger import MetricLogger\n",
    "from hisup.utils.checkpoint import DetectronCheckpointer\n",
    "\n",
    "def inference_single(cfg, model, image, device):\n",
    "    \n",
    "    transform = build_transform(cfg)\n",
    "    \n",
    "    h_stride, w_stride = 400, 400\n",
    "    h_crop, w_crop = cfg.DATASETS.ORIGIN.HEIGHT, cfg.DATASETS.ORIGIN.WIDTH\n",
    "    h_img, w_img, _ = image.shape\n",
    "    h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n",
    "    w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n",
    "    pred_whole_img = np.zeros([h_img, w_img], dtype=np.float32)\n",
    "    count_mat = np.zeros([h_img, w_img])\n",
    "    juncs_whole_img = []\n",
    "    \n",
    "    patch_weight = np.ones((h_crop + 2, w_crop + 2))\n",
    "    patch_weight[0,:] = 0\n",
    "    patch_weight[-1,:] = 0\n",
    "    patch_weight[:,0] = 0\n",
    "    patch_weight[:,-1] = 0\n",
    "    \n",
    "    patch_weight = scipy.ndimage.distance_transform_edt(patch_weight)\n",
    "    patch_weight = patch_weight[1:-1,1:-1]\n",
    "\n",
    "    for h_idx in tqdm(range(h_grids), desc='processing on image'):\n",
    "        for w_idx in range(w_grids):\n",
    "            y1 = h_idx * h_stride\n",
    "            x1 = w_idx * w_stride\n",
    "            y2 = min(y1 + h_crop, h_img)\n",
    "            x2 = min(x1 + w_crop, w_img)\n",
    "            y1 = max(y2 - h_crop, 0)\n",
    "            x1 = max(x2 - w_crop, 0)\n",
    "            \n",
    "            crop_img = image[y1:y2, x1:x2, :]\n",
    "            crop_img_tensor = transform(crop_img.astype(float))[None].to(device)\n",
    "            \n",
    "            meta = {\n",
    "                'height': crop_img.shape[0],\n",
    "                'width': crop_img.shape[1],\n",
    "                'pos': [x1, y1, x2, y2]\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output, _ = model(crop_img_tensor, [meta])\n",
    "                output = to_single_device(output, 'cpu')\n",
    "\n",
    "            juncs_pred = output['juncs_pred'][0]\n",
    "            mask_pred = output['mask_pred'][0]\n",
    "            juncs_pred += [x1, y1]\n",
    "            juncs_whole_img.extend(juncs_pred.tolist())\n",
    "            mask_pred *= patch_weight\n",
    "            pred_whole_img += np.pad(mask_pred,\n",
    "                                ((int(y1), int(pred_whole_img.shape[0] - y2)),\n",
    "                                (int(x1), int(pred_whole_img.shape[1] - x2))))\n",
    "            count_mat[y1:y2, x1:x2] += patch_weight\n",
    "\n",
    "    juncs_whole_img = np.array(juncs_whole_img)\n",
    "    pred_whole_img = pred_whole_img / count_mat\n",
    "\n",
    "    return juncs_whole_img, pred_whole_img\n",
    "\n",
    "\n",
    "def inference_in_batches(cfg, model, image_patches, device):\n",
    "\n",
    "    for i in range(image_patches.shape[0]):\n",
    "        \n",
    "        image = image_patches[i]\n",
    "        juncs_whole_img_single, pred_whole_img_single = inference_single(cfg, model, image, device)\n",
    "        juncs_whole_img_single_ex = np.expand_dims(juncs_whole_img_single, axis=0)\n",
    "        pred_whole_img_single_ex = np.expand_dims(pred_whole_img_single, axis=0)\n",
    "\n",
    "        if i == 0:\n",
    "            juncs_whole_img_all = juncs_whole_img_single_ex\n",
    "            pred_whole_img_all = pred_whole_img_single_ex\n",
    "\n",
    "        else:\n",
    "            juncs_whole_img_all = np.concatenate((juncs_whole_img_all, juncs_whole_img_single_ex), axis=0)\n",
    "            pred_whole_img_all = np.concatenate((pred_whole_img_all, pred_whole_img_single_ex), axis=0)\n",
    "\n",
    "    return juncs_whole_img_all, pred_whole_img_all\n",
    "\n",
    "\n",
    "def convert_mask_to_polygons(juncs_whole_img, pred_whole_img):\n",
    "    # match junction and seg results\n",
    "    polygons = []\n",
    "    props = regionprops(label(pred_whole_img > 0.5))\n",
    "    for prop in tqdm(props, leave=False, desc='polygon generation'):\n",
    "        y1, x1, y2, x2 = prop.bbox\n",
    "        bbox = [x1, y1, x2, y2]\n",
    "        select_juncs = juncs_in_bbox(bbox, juncs_whole_img, expand=8)\n",
    "        poly, juncs_sa, _, _, juncs_index = generate_polygon(prop, pred_whole_img, \\\n",
    "                                                                    select_juncs, pid=0, test_inria=True)\n",
    "        if juncs_sa.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(juncs_index) == 1:\n",
    "            polygons.append(Polygon(poly))\n",
    "        else:\n",
    "            poly_ = Polygon(poly[juncs_index[0]], \\\n",
    "                            [poly[idx] for idx in juncs_index[1:]])\n",
    "            polygons.append(poly_)\n",
    "    \n",
    "    return polygons\n",
    "\n",
    "def get_pretrained_model_FT(cfg, dataset, device, path_model, pretrained=True):\n",
    "    \n",
    "    model = BuildingDetector(cfg, test=True)\n",
    "    # if pretrained:\n",
    "    #     url = PRETRAINED[dataset]\n",
    "    #     state_dict = torch.hub.load_state_dict_from_url(url, map_location=device, progress=True)\n",
    "    # state_dict = {k[7:]:v for k,v in state_dict['model'].items() if k[0:7] == 'module.'}\n",
    "    # model.load_state_dict(state_dict)\n",
    "    # state_dict = model.load_state_dict(path_model)\n",
    "    # state_dict = {k[7:]:v for k,v in state_dict['model'].items() if k[0:7] == 'module.'}\n",
    "    # model.load_state_dict(state_dict)\n",
    "    state_dict = torch.load(path_model)\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    model = model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(path, \"data\")\n",
    "path_test = os.path.join(path_data, \"outputs\")\n",
    "img_path = \"path to your geotif file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ9GXoTboIV6"
   },
   "source": [
    "### Crop a geotiff raster whose width and height can be divided by 512 (may be optional), and save it as a new geotiff raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14799,
     "status": "ok",
     "timestamp": 1684740502496,
     "user": {
      "displayName": "Yunya Gao",
      "userId": "05443726764578958815"
     },
     "user_tz": -120
    },
    "id": "UIJfy9Pak4Ge",
    "outputId": "8d846943-9de6-4909-d447-1e935422cbb2"
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "src = rasterio.open(img_path)\n",
    "width = src.width\n",
    "height = src.height\n",
    "\n",
    "patchsize = 512\n",
    "width_new = int(width / patchsize) * patchsize\n",
    "height_new = int(height / patchsize) * patchsize\n",
    "print(width, height, width_new, height_new)\n",
    "\n",
    "xmin, xmax = 0, width_new\n",
    "ymin, ymax = 0, height_new\n",
    "xoff, yoff = 1, 1\n",
    "\n",
    "# Create a Window and calculate the transform from the source dataset    \n",
    "window = Window(xoff, yoff, width_new, height_new)\n",
    "transform = src.window_transform(window)\n",
    "\n",
    "profile = src.profile\n",
    "print(profile)\n",
    "profile.update({\n",
    "    'height': height_new, # it can be confusing\n",
    "    'width': width_new,\n",
    "    'transform': transform})\n",
    "print(profile)\n",
    "\n",
    "img_out_path = os.path.join(path_test, \"image_test_cropped.tif\")\n",
    "with rasterio.open(img_out_path, 'w', **profile) as dst:\n",
    "    # Read the data from the window and write it to the output raster\n",
    "    dst.write(src.read(window=window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLF4eUXsktnU"
   },
   "source": [
    "### Load pretrained model from your data or Infria or Crowdai data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12982,
     "status": "ok",
     "timestamp": 1684740581963,
     "user": {
      "displayName": "Yunya Gao",
      "userId": "05443726764578958815"
     },
     "user_tz": -120
    },
    "id": "73hoFK9Niac5",
    "outputId": "f82542d0-6a10-43be-b672-82a1ea1b7dde"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset = 'crowdai' # crowdai inria\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "img_path = os.path.join(path_test, \"image_test_cropped.tif\")\n",
    "image = io.imread(img_path)[:, :, :3]\n",
    "\n",
    "H, W = patchsize, patchsize # patchsize = 512\n",
    "cfg.DATASETS.ORIGIN.HEIGHT = 512 if H > 512 else H\n",
    "cfg.DATASETS.ORIGIN.WIDTH = 512 if W > 512 else W\n",
    "\n",
    "# here I \n",
    "# load your pretrained model\n",
    "path_model = \"path to pretrained model(.pth)\"\n",
    "\n",
    "model = get_pretrained_model_FT(cfg, dataset, device, path_model, pretrained=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference, creating mask raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "from skimage.measure import label, regionprops\n",
    "from hisup.utils.polygon import generate_polygon, juncs_in_bbox\n",
    "from hisup.utils.visualizer import viz_inria\n",
    "\n",
    "transform = build_transform(cfg)\n",
    "\n",
    "h_stride, w_stride = 400, 400\n",
    "h_crop, w_crop = cfg.DATASETS.ORIGIN.HEIGHT, cfg.DATASETS.ORIGIN.WIDTH\n",
    "h_img, w_img, _ = image.shape\n",
    "h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n",
    "w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n",
    "pred_whole_img = np.zeros([h_img, w_img], dtype=np.float32)\n",
    "count_mat = np.zeros([h_img, w_img])\n",
    "juncs_whole_img = []\n",
    "\n",
    "patch_weight = np.ones((h_crop + 2, w_crop + 2))\n",
    "patch_weight[0,:] = 0\n",
    "patch_weight[-1,:] = 0\n",
    "patch_weight[:,0] = 0\n",
    "patch_weight[:,-1] = 0\n",
    "\n",
    "patch_weight = scipy.ndimage.distance_transform_edt(patch_weight)\n",
    "patch_weight = patch_weight[1:-1,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h_idx in tqdm(range(h_grids), desc='processing on image'):\n",
    "    for w_idx in range(w_grids):\n",
    "        y1 = h_idx * h_stride\n",
    "        x1 = w_idx * w_stride\n",
    "        y2 = min(y1 + h_crop, h_img)\n",
    "        x2 = min(x1 + w_crop, w_img)\n",
    "        y1 = max(y2 - h_crop, 0)\n",
    "        x1 = max(x2 - w_crop, 0)\n",
    "\n",
    "        crop_img = image[y1:y2, x1:x2, :]\n",
    "        crop_img_tensor = transform(crop_img.astype(float))[None].to(device)\n",
    "\n",
    "        meta = {\n",
    "            'height': crop_img.shape[0],\n",
    "            'width': crop_img.shape[1],\n",
    "            'pos': [x1, y1, x2, y2]\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(crop_img_tensor, [meta])\n",
    "            output = to_single_device(output, 'cpu')\n",
    "\n",
    "        juncs_pred = output['juncs_pred'][0]\n",
    "        mask_pred = output['mask_pred'][0]\n",
    "        juncs_pred += [x1, y1]\n",
    "        juncs_whole_img.extend(juncs_pred.tolist())\n",
    "        mask_pred *= patch_weight\n",
    "        pred_whole_img += np.pad(mask_pred,\n",
    "                            ((int(y1), int(pred_whole_img.shape[0] - y2)),\n",
    "                            (int(x1), int(pred_whole_img.shape[1] - x2))))\n",
    "        count_mat[y1:y2, x1:x2] += patch_weight\n",
    "\n",
    "juncs_whole_img = np.array(juncs_whole_img)\n",
    "pred_whole_img = pred_whole_img / count_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicted mask and shapefile with geoinformation\n",
    "\n",
    "import pathlib\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "path_out_img = os.path.join(path_test, \"Pred_Mask\")\n",
    "path_out_shp = os.path.join(path_test, \"Pred_Shp\")\n",
    "\n",
    "pathlib.Path(path_out_img).mkdir(parents=True, exist_ok=True) \n",
    "pathlib.Path(path_out_shp).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# https://geobgu.xyz/py/rasterio.html\n",
    "with rasterio.open(img_path) as src:\n",
    "    ras_meta = src.profile\n",
    "    crs = src.crs # for vector\n",
    "    ras_meta[\"count\"] = 1 # for raster\n",
    "    ras_meta[\"dtype\"] = \"float32\" \n",
    "\n",
    "pred_whole_img_ = np.expand_dims(pred_whole_img[...], axis=0)\n",
    "\n",
    "path_out_img = \"path to output mask (.tif)\"\n",
    "with rasterio.open(path_out_img, 'w', **ras_meta) as dst:\n",
    "    dst.write(pred_whole_img_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match junction and seg results\n",
    "polygons = []\n",
    "thres = 0.5 # it can be an important variable, 0.5 may not be always the best option.\n",
    "\n",
    "props = regionprops(label(pred_whole_img > thres))\n",
    "for prop in tqdm(props, leave=False, desc='polygon generation'):\n",
    "    y1, x1, y2, x2 = prop.bbox\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    select_juncs = juncs_in_bbox(bbox, juncs_whole_img, expand=8)\n",
    "    poly, juncs_sa, _, _, juncs_index = generate_polygon(prop, pred_whole_img, select_juncs, pid=0, test_inria=True)\n",
    "                                                                \n",
    "    if juncs_sa.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    if len(juncs_index) == 1:\n",
    "        polygons.append(Polygon(poly))\n",
    "    else:\n",
    "        poly_ = Polygon(poly[juncs_index[0]], \\\n",
    "                        [poly[idx] for idx in juncs_index[1:]])\n",
    "        polygons.append(poly_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add geoinformation to the created polygons\n",
    "\n",
    "from shapely.affinity import scale\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries\n",
    "\n",
    "img_path = os.path.join(path_test, \"image_test_cropped.tif\")\n",
    "src = rasterio.open(img_path)\n",
    "left, bottom = src.bounds.left, src.bounds.bottom\n",
    "    \n",
    "# it is important to set the resolution when creating the final polygons in shapefile\n",
    "resolution = 0.1\n",
    "\n",
    "for poly in polygons:\n",
    "\n",
    "    x, y = poly.exterior.coords.xy\n",
    "    x_ = x.tolist()\n",
    "    y_ = y.tolist()\n",
    "\n",
    "    x__ = [i*resolution + left for i in x_]\n",
    "    y__ = [j*resolution + bottom for j in y_]\n",
    "    \n",
    "    poly_geo = Polygon(zip(x__, y__))\n",
    "    polygons_geo.append(poly_geo)\n",
    "\n",
    "len(polygons_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m polygons_gpd \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241m.\u001b[39mGeoSeries(polygons_geo)\n\u001b[1;32m      3\u001b[0m origin \u001b[38;5;241m=\u001b[39m ((src\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m+\u001b[39m src\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mright)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, (src\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mtop \u001b[38;5;241m+\u001b[39m src\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mbottom)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m flip \u001b[38;5;241m=\u001b[39m GeoSeries\u001b[38;5;241m.\u001b[39mscale(polygons_gpd, xfact\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, yfact\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, zfact\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, origin\u001b[38;5;241m=\u001b[39morigin) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpd' is not defined"
     ]
    }
   ],
   "source": [
    "# add geoinformation to the created polygons\n",
    "\n",
    "from shapely.affinity import scale\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries\n",
    "\n",
    "img_path = os.path.join(path_test, \"image_test_cropped.tif\")\n",
    "src = rasterio.open(img_path)\n",
    "left, bottom = src.bounds.left, src.bounds.bottom\n",
    "    \n",
    "# it is important to set the resolution when creating the final polygons in shapefile\n",
    "resolution = 0.1\n",
    "\n",
    "for poly in polygons:\n",
    "\n",
    "    x, y = poly.exterior.coords.xy\n",
    "    x_ = x.tolist()\n",
    "    y_ = y.tolist()\n",
    "\n",
    "    x__ = [i*resolution + left for i in x_]\n",
    "    y__ = [j*resolution + bottom for j in y_]\n",
    "    \n",
    "    poly_geo = Polygon(zip(x__, y__))\n",
    "    polygons_geo.append(poly_geo)\n",
    "\n",
    "len(polygons_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "fig.set_size_inches(25, 25)\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in flip:   \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=0.5, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eqsmMLeE-NgY",
    "9iqFVND1pjRS",
    "IQ9GXoTboIV6",
    "AB73bX17oNwV",
    "DLF4eUXsktnU",
    "Eqw_MdG3u6FZ",
    "Vip7Q03Tw13A"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
